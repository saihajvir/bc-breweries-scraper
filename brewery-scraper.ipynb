{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Starting BC Ale Trail complete brewery scraper...\n",
      "Fetching breweries list from https://bcaletrail.ca/breweries/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 275\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting BC Ale Trail complete brewery scraper...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Scrape all breweries\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m breweries_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_all_breweries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m breweries_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# Display summary\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScraping summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mscrape_all_breweries\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching breweries list from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve the page. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/requests/sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/urllib3/response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/urllib3/response.py:831\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 831\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m    833\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    834\u001b[0m )\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/urllib3/response.py:784\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    782\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[0;32m--> 784\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_left\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/http/client.py:612\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    606\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 612\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m    614\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "def scrape_all_breweries():\n",
    "    \"\"\"\n",
    "    Scrapes data for all breweries from the BC Ale Trail website.\n",
    "    Returns a pandas DataFrame with the extracted information.\n",
    "    \"\"\"\n",
    "    # Base URL for the breweries page\n",
    "    url = \"https://bcaletrail.ca/breweries/\"\n",
    "    \n",
    "    # Send a request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetching breweries list from {url}\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all brewery elements\n",
    "    brewery_cards = soup.select(\".listing-item\")\n",
    "    \n",
    "    if not brewery_cards:\n",
    "        print(\"No brewery cards found. The website structure may have changed.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(brewery_cards)} breweries to scrape\")\n",
    "    \n",
    "    # List to store all brewery data\n",
    "    all_breweries = []\n",
    "    \n",
    "    # Process each brewery\n",
    "    for i, brewery_card in enumerate(brewery_cards):\n",
    "        try:\n",
    "            brewery = {}\n",
    "            \n",
    "            # Extract brewery name\n",
    "            name_element = brewery_card.select_one(\".listing-title\")\n",
    "            brewery[\"name\"] = name_element.text.strip() if name_element else \"N/A\"\n",
    "\n",
    "            # Extract brewery city\n",
    "            city_element = brewery_card.select_one(\".location\")\n",
    "            brewery[\"city\"] = city_element.text.strip() if city_element else \"N/A\"\n",
    "\n",
    "            # Extract brewery type\n",
    "            brewery_type_element = brewery_card.select_one(\".features\")\n",
    "            if brewery_type_element:\n",
    "                # Split features by the pipe character and strip each feature\n",
    "                features_text = brewery_type_element.text.strip()\n",
    "                features_list = [feature.strip() for feature in features_text.split(\"|\")]\n",
    "                brewery[\"brewery_type\"] = features_list\n",
    "            else:\n",
    "                brewery[\"brewery_type\"] = []\n",
    "\n",
    "            # Extract brewery URL\n",
    "            link_element = brewery_card.select_one(\"a\")\n",
    "            brewery[\"url\"] = link_element[\"href\"] if link_element and \"href\" in link_element.attrs else \"N/A\"\n",
    "            \n",
    "            # Progress indicator\n",
    "            print(f\"\\nProcessing brewery {i+1}/{len(brewery_cards)}: {brewery['name']}\")\n",
    "            \n",
    "            # Additional data from brewery detail page\n",
    "            if brewery[\"url\"] != \"N/A\":\n",
    "                print(f\"Fetching detailed information from {brewery['url']}\")\n",
    "                detail_data = scrape_brewery_detail(brewery[\"url\"])\n",
    "                brewery.update(detail_data)\n",
    "                \n",
    "                # Add a delay to avoid overloading the server\n",
    "                if i < len(brewery_cards) - 1:  # No need to delay after the last brewery\n",
    "                    delay = random.uniform(1.0, 3.0)\n",
    "                    print(f\"Waiting {delay:.2f} seconds before next request...\")\n",
    "                    time.sleep(delay)\n",
    "            \n",
    "            all_breweries.append(brewery)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing brewery: {str(e)}\")\n",
    "            continue  # Skip to the next brewery if there's an error\n",
    "    \n",
    "    if not all_breweries:\n",
    "        print(\"No breweries were successfully scraped.\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_breweries)\n",
    "    \n",
    "    print(f\"\\nSuccessfully scraped {len(all_breweries)} breweries\")\n",
    "    return df\n",
    "\n",
    "def scrape_brewery_detail(url):\n",
    "    \"\"\"\n",
    "    Scrapes detailed information from a brewery's specific page.\n",
    "    \"\"\"\n",
    "    detail_info = {\n",
    "        \"address\": \"N/A\",\n",
    "        \"address_2\": \"N/A\",\n",
    "        \"postal_code\": \"N/A\",\n",
    "        \"state_province\": \"N/A\",\n",
    "        \"phone\": \"N/A\",\n",
    "        \"website_url\": \"N/A\",  # Fixed typo in field name\n",
    "        \"social_media\": \"N/A\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve detail page. Status code: {response.status_code}\")\n",
    "            return detail_info\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Extract address\n",
    "        address_element = soup.select_one(\".address a\")\n",
    "        if address_element:\n",
    "            full_address = address_element.text.strip()\n",
    "            address_parts = parse_address(full_address)\n",
    "            detail_info.update(address_parts)\n",
    "        \n",
    "        # Extract phone\n",
    "        phone_element = soup.select_one(\".tel a\")\n",
    "        if phone_element:\n",
    "            raw_phone = phone_element.text.strip()\n",
    "            detail_info[\"phone\"] = clean_phone_number(raw_phone)\n",
    "        \n",
    "        # Extract website\n",
    "        website_element = soup.select_one(\".listing-links a\")\n",
    "        if website_element and \"href\" in website_element.attrs:\n",
    "            detail_info[\"website_url\"] = website_element[\"href\"]  # Fixed typo in field name\n",
    "        \n",
    "        # # Extract description\n",
    "        # description_element = soup.select_one(\".brewery-description\")\n",
    "        # if description_element:\n",
    "        #     detail_info[\"description\"] = description_element.text.strip()\n",
    "        \n",
    "        # Extract social media links\n",
    "        social_media_elements = soup.select(\".list-social-item a\")\n",
    "        if social_media_elements:\n",
    "            social_links = [link[\"href\"] for link in social_media_elements if \"href\" in link.attrs]\n",
    "            detail_info[\"social_media\"] = social_links\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping detail page: {str(e)}\")\n",
    "    \n",
    "    return detail_info\n",
    "\n",
    "def parse_address(full_address):\n",
    "    \"\"\"\n",
    "    Parse the full address string into components:\n",
    "    - address (main street address)\n",
    "    - address_2 (unit/suite number if present)\n",
    "    - postal_code (Canadian postal code)\n",
    "    - province (BC)\n",
    "    \"\"\"\n",
    "    address_components = {\n",
    "        \"address\": \"N/A\",\n",
    "        \"address_2\": \"N/A\", \n",
    "        \"postal_code\": \"N/A\",\n",
    "        \"state_province\": \"N/A\",\n",
    "        \"country\": \"N/A\"\n",
    "    }\n",
    "    \n",
    "    # Default province to BC since these are BC breweries\n",
    "    address_components[\"state_province\"] = \"BC\"\n",
    "    address_components[\"country\"] = \"Canada\"\n",
    "    \n",
    "\n",
    "    # Split the address by commas and clean each part\n",
    "    parts = [part.strip() for part in full_address.split(\",\")]\n",
    "    \n",
    "    # If there's at least one part, it's the main address\n",
    "    if parts:\n",
    "        main_address = parts[0]\n",
    "        \n",
    "        # Check for unit/suite in the main address\n",
    "        unit_indicators = [\"Unit\", \"Suite\", \"#\", \"Ste\", \"Apt\"]\n",
    "        for indicator in unit_indicators:\n",
    "            if indicator in main_address:\n",
    "                # Try to split at the indicator\n",
    "                split_point = main_address.find(indicator)\n",
    "                if split_point >= 0:\n",
    "                    address_2 = main_address[:split_point+len(indicator)].strip()\n",
    "                    # Find the end of the unit number\n",
    "                    for i in range(split_point+len(indicator), len(main_address)):\n",
    "                        if not main_address[i].isdigit() and main_address[i] not in [' ', '-', '.']:\n",
    "                            break\n",
    "                    address_2 += main_address[split_point+len(indicator):i].strip()\n",
    "                    main_address = main_address[i:].strip()\n",
    "                    address_components[\"address_2\"] = address_2\n",
    "                break\n",
    "        \n",
    "        address_components[\"address\"] = main_address.strip()\n",
    "    \n",
    "    # Look for postal code in the last part (city, province postal_code)\n",
    "    if len(parts) > 1:\n",
    "        last_part = parts[-1]\n",
    "        \n",
    "        # Canadian postal codes are in format A1A 1A1\n",
    "        import re\n",
    "        postal_match = re.search(r'[A-Za-z]\\d[A-Za-z]\\s?\\d[A-Za-z]\\d', last_part)\n",
    "        if postal_match:\n",
    "            address_components[\"postal_code\"] = postal_match.group(0).strip()\n",
    "            \n",
    "            # Remove postal code from the last part\n",
    "            remaining = last_part.replace(postal_match.group(0), \"\").strip()\n",
    "            \n",
    "            # If there's remaining text, it might contain the province\n",
    "            if \"BC\" in remaining:\n",
    "                # If BC is explicitly mentioned, remove it from the address and add to province\n",
    "                address_components[\"state_province\"] = \"BC\"\n",
    "            elif \"British Columbia\" in remaining:\n",
    "                address_components[\"state_province\"] = \"BC\"\n",
    "    \n",
    "    return address_components\n",
    "\n",
    "def clean_phone_number(phone):\n",
    "    \"\"\"\n",
    "    Clean phone number by removing parentheses, dashes, spaces, and other non-numeric characters.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Keep only digits\n",
    "    return re.sub(r'[^0-9]', '', phone)\n",
    "\n",
    "def save_data(df, file_type=\"csv\"):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a file.\n",
    "    Supported file types: csv, excel, json\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to save\")\n",
    "        return\n",
    "    \n",
    "    filename = f\"bc_breweries_complete.{file_type}\"\n",
    "    \n",
    "    if file_type.lower() == \"csv\":\n",
    "        df.to_csv(filename, index=False, encoding=\"utf-8\")\n",
    "    elif file_type.lower() == \"excel\":\n",
    "        df.to_excel(filename, index=False)\n",
    "    elif file_type.lower() == \"json\":\n",
    "        # Use json module directly instead of pandas' to_json to avoid escape slashes\n",
    "        import json\n",
    "        \n",
    "        # Convert DataFrame to a list of dictionaries\n",
    "        records = df.to_dict(orient='records')\n",
    "        \n",
    "        # Write to file with ensure_ascii=False to properly handle non-ASCII characters\n",
    "        # and without escaping forward slashes\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(records, f, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_type}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Data successfully saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting BC Ale Trail complete brewery scraper...\")\n",
    "    \n",
    "    # Scrape all breweries\n",
    "    breweries_df = scrape_all_breweries()\n",
    "    \n",
    "    if breweries_df is not None:\n",
    "        # Display summary\n",
    "        print(\"\\nScraping summary:\")\n",
    "        print(f\"Total breweries scraped: {len(breweries_df)}\")\n",
    "        print(f\"Columns in dataset: {', '.join(breweries_df.columns)}\")\n",
    "        \n",
    "        # Save data to files\n",
    "        save_data(breweries_df, \"json\")\n",
    "        save_data(breweries_df, \"csv\")\n",
    "    \n",
    "    print(\"Scraping completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
